# DenseFusion-note
Note about DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion

## 6D Pose

6D Pose是一个[R|t]矩阵，其中R是旋转矩阵，t是平移矩阵，结合起来描述了一个物体在相机坐标系下的位置与姿态。



## 以往方法的缺陷

- 分别提取RGB与D的信息，或把D作为一个额外的通道加入到输入中，单纯地进行连接，失去了RGB和三维点云之间固有的映射关系。
- 在严重遮挡的情况下效果很差。
- 光照影响大。
- 需要很复杂的后续优化步骤(ICP)。



## 本文的贡献

- 合理地结合了RGBD的信息。
- 利用一个包含在网络结构中的迭代优化处理代替了ICP步骤。



## 网络结构

网络主要分成5个部分。

1. Object Segmentation。输入RGB图形进行语义分割，识别出其中的物体。这里用的分割方法是"Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes"中的方法。再根据分割结果将图像裁剪，留下需要的物体部分，这里图像大小为`H*W`，所以一共有`H*W`个像素点。把这个裁剪后的图片送入CNN网络中，提取出`H*W*drgb`数量的特征值，也就是每个像素点有`drgb`个特征值。

2. 利用上面的分割结果，对应深度图，生成一个物体的点云，假设点云一共有`P`个点。把这个点云送入PointNet网络中(用了average pooling而不是原本的max pooling)，提取出`P*dgeo`个特征值，也就是每个点有`dgeo`个特征值。
3. 因为严重遮挡、语义分割不精确等原因，如果盲目地融合1、2的特征值反而会降低网络的准确性。所以在融合的时候，把点云中的`P`个点对应到RGB中的`P`个点(利用已知的相机内参)，找到它们在RGB提取的特征向量，即`P*drgb`。将它们一一对应连接起来，那么现在一共有`P*(drgb+dgeo)`这么多个特征值，把这个作为输入送入一个MLP中，在经过一个average pooling，得到一个固定长度的global feature。
4. 3中得到的global feature连接到之前的`P*(drgb+dgeo)`后面，所以是`P*(drgb+dgeo+global feature)`。这个作为输出送到最后的网络中，每一个特征向量都会得到一个结果，每一个结果是一个`[R|t]`矩阵的值和一个confidence值，一共有P个结果。其中confidence最大的结果会被选择。
5. 优化模块。假设现在出来的结果是`[R0|t0]`，则把2中的点云就按照这个位姿进行旋转，并送回网络中从第3步开始继续进行，会输出一个`[R1|t1]`，以此类推，经过k次迭代会得到一个最终的矩阵，这个矩阵是之前每一轮迭代矩阵的乘积。这个优化模块的训练要在之前的网络收敛之后才可以开始。



## Loss函数

### 对于非对称物体

从物体模型上采样一些点，以其中一个点为例，得到这个点的坐标`x`和Groud Truth位姿`[R|t]`，预测出来的位置是`[R'|t']`，则位姿估计的loss定义为`x`经过`[R|t]`变换后的点和`x`经过`[R'|t']`变换后的点之间的距离。若采样`M`个点，那么就需要把这`M`个loss相加再除以`M`得到这个预测结果的平均loss。

### 对于对称物体

对称物体可能存在多个正则坐标系，会导致网络没有明确的学习目标。所以Groud Truth上的点`x`不可以直接和利用预测出来位姿获得的模型上的点对应，而需要找到预测模型上离`x`最近的点，计算这两点之间的距离。同样采样`M`个点的话就需要相加再除以`M`。

### 总体loss

上面提到的都是一个预测结果的loss，而每次会产生`P`个预测结果，所以需要把这些结果再相加取平均。但是因为输出还有一个置信度confidence `c`，所以考虑到置信度的影响，需要修改总体的loss为：

$L=\cfrac{1}{N}\sum L_i^Pc_i-w\log (ci)$，其中N是从`P`个特征向量中随机采样的数量，w是一个超参数。

当某一个预测结果的置信度较低时，这个预测结果受到的惩罚就会比较少，但是这个置信度收到的惩罚就会较大。
